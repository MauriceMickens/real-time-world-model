{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 3: Host→Device Memcpy Scaling\n",
        "\n",
        "**Focus:** Understanding memory-bound operations and scaling behavior\n",
        "\n",
        "## Objectives\n",
        "- Profile Host→Device memcpy operations\n",
        "- Understand how memory transfer scales with size\n",
        "- Compare memory bandwidth (GB/s) across different sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import nvtx\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA not available\"\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "DTYPE = torch.float16\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"DTYPE: {DTYPE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper: CUDA Event Timing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WARMUP = 5\n",
        "ITERS = 10\n",
        "\n",
        "def time_cuda(fn, stream=None):\n",
        "    \"\"\"Time a CUDA operation using events.\"\"\"\n",
        "    if stream is None:\n",
        "        stream = torch.cuda.current_stream()\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(WARMUP):\n",
        "        fn()\n",
        "    stream.synchronize()\n",
        "\n",
        "    # Timed\n",
        "    start.record(stream)\n",
        "    for _ in range(ITERS):\n",
        "        fn()\n",
        "    end.record(stream)\n",
        "    end.synchronize()\n",
        "\n",
        "    ms = start.elapsed_time(end) / ITERS\n",
        "    return ms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment: H2D Memcpy Scaling\n",
        "\n",
        "Measure transfer time and bandwidth for different tensor sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SIZES = [1024, 2048, 4096, 8192]\n",
        "\n",
        "print(\"=== H2D memcpy scaling ===\")\n",
        "print(f\"{'N':>6}  {'ms':>10}  {'MB':>10}  {'GB/s':>8}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "results = []\n",
        "\n",
        "for N in SIZES:\n",
        "    with nvtx.annotate(f\"h2d_alloc_{N}\"):\n",
        "        # Pinned host memory (faster for transfers)\n",
        "        x_cpu = torch.empty((N, N), dtype=DTYPE, pin_memory=True).normal_()\n",
        "        x_gpu = torch.empty((N, N), dtype=DTYPE, device=DEVICE)\n",
        "\n",
        "    def h2d():\n",
        "        with nvtx.annotate(f\"h2d_{N}\"):\n",
        "            x_gpu.copy_(x_cpu, non_blocking=True)\n",
        "\n",
        "    ms = time_cuda(h2d)\n",
        "    mb = x_cpu.numel() * x_cpu.element_size() / 1e6\n",
        "    gbps = (mb / 1000.0) / (ms / 1000.0)  # GB/s\n",
        "    \n",
        "    results.append((N, ms, mb, gbps))\n",
        "    print(f\"{N:6d}  {ms:10.3f}  {mb:10.3f}  {gbps:8.2f}\")\n",
        "\n",
        "print(\"\\nNote: Size scales as N², so larger N means more data transferred.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis\n",
        "\n",
        "**Questions to answer:**\n",
        "1. How does transfer time scale with size? (linear? quadratic?)\n",
        "2. Is bandwidth constant across sizes? Why or why not?\n",
        "3. What would you expect to see for D2H (Device→Host) transfers?\n",
        "\n",
        "_Record your observations here after running the experiments._"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
